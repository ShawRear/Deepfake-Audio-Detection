{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, accuracy_score\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom torchvision import datasets, transforms, models\nfrom torchvision.models import ResNet101_Weights\nimport torch.nn as nn\n# Hyperparameters and paths (modify as necessary)\ndropout_rate = 0.6\nbatch_size = 32\nnum_classes = 2 # Modify based on your problem\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# Transforms for test dataset (same as validation)\nval_transforms = transforms.Compose([\ntransforms.Resize(256),\ntransforms.CenterCrop(224),\ntransforms.ToTensor(),\ntransforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n# Load the test dataset\ntest_dataset = datasets.ImageFolder(root='/kaggle/input/audiospec/output_spectrograms/output_spectrograms', transform=val_transforms)\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n# Model: Initialize the same model architecture\nmodel = models.resnet101(weights=ResNet101_Weights.DEFAULT)\nnum_features = model.fc.in_features\nmodel.fc = nn.Sequential(\nnn.Dropout(dropout_rate),\nnn.Linear(num_features, num_classes)\n)\n# Load the saved model weights\nmodel_path = '/kaggle/input/res101haha/pytorch/default/1/resnet101_model_weights.pth' # Update this with the actual path\nmodel.load_state_dict(torch.load(model_path)) # Load the model weights\nmodel.to(device) # Send the model to the appropriate device (GPU or CPU)\n# Set the model to evaluation mode\nmodel.eval()\n# Function to evaluate on the test set and compute metrics\ndef evaluate_on_test_set(model, test_loader):\nall_labels = []\nall_predictions = []\nwith torch.no_grad(): # Disable gradient calculation for inference\nfor images, labels in test_loader:\nimages, labels = images.to(device), labels.to(device)\noutputs = model(images)\n_, predicted = torch.max(outputs.data, 1) # Get predictions\n# Store true labels and predictions\nall_labels.extend(labels.cpu().numpy())\nall_predictions.extend(predicted.cpu().numpy())\n# Calculate precision, recall, F1 score, and accuracy\nprecision = precision_score(all_labels, all_predictions, average='macro')\nrecall = recall_score(all_labels, all_predictions, average='macro')\nf1 = f1_score(all_labels, all_predictions, average='macro')\naccuracy = accuracy_score(all_labels, all_predictions)\nprint(f'Precision (Macro): {precision:.4f}')\nprint(f'Recall (Macro): {recall:.4f}')\nprint(f'F1 Score (Macro): {f1:.4f}')\nprint(f'Accuracy: {accuracy:.4f}')\nreturn all_labels, all_predictions\n# Function to plot confusion matrix\ndef plot_confusion_matrix(all_labels, all_predictions, class_names):\ncm = confusion_matrix(all_labels, all_predictions)\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.title('Confusion Matrix')\nplt.show()\n# Run the inference on the test set\nall_test_labels, all_test_predictions = evaluate_on_test_set(model, test_loader)\n# Plot confusion matrix\nplot_confusion_matrix(all_test_labels, all_test_predictions, class_names=test_dataset.classes)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null}]}